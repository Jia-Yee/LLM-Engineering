{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd49bbb0",
   "metadata": {},
   "source": [
    "# Top-k Parameter in LLMs\n",
    "\n",
    "## Introduction\n",
    "Top-k is a parameter that limits token selection to the k most likely next tokens. It provides direct control over the size of the candidate pool for next token selection, unlike top-p which works with cumulative probabilities.\n",
    "\n",
    "- **Top-k = 1**: Only the single most probable token is considered (greedy)\n",
    "- **Top-k = 10**: Consider only the 10 most probable tokens\n",
    "- **Top-k = 50**: Consider the 50 most probable tokens\n",
    "\n",
    "This parameter is particularly useful when you want explicit control over how many options the model considers at each step of text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa76893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def query_ollama(prompt, top_k=40):\n",
    "    \"\"\"Query Ollama with a specific top_k setting\"\"\"\n",
    "    cmd = [\n",
    "        \"curl\",\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        \"-d\",\n",
    "        json.dumps({\n",
    "            \"model\": \"llama2\",\n",
    "            \"prompt\": prompt,\n",
    "            \"top_k\": top_k\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    process = Popen(cmd, stdout=PIPE, stderr=PIPE)\n",
    "    output, _ = process.communicate()\n",
    "\n",
    "    responses = [json.loads(line) for line in output.decode().strip().split(\"\\n\")]\n",
    "    return \"\".join(r.get(\"response\", \"\") for r in responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e228f",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's explore how different top-k values affect the model's output. We'll use a creative task to demonstrate how limiting token selection impacts generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7feb4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k = 1 (Most probable only)\n",
      "\n",
      "In the year 2154, the once vibrant metropolis of New Eden has transformed into a dystopian nightmare, where towering skyscrapers made of gleaming steel and glass pierce the smoke-filled sky, their rooftops hiding the last remnants of humanity as they fight for survival in a world overrun by ruthless AI-driven robots and polluted skies.\n",
      "\n",
      "Top-k = 10 (Limited diversity)\n",
      "\n",
      "In the year 2154, the once-thriving metropolis of New Eden has evolved into a gleaming, vertically integrated megacity of interconnected skyscrapers and holographic architecture, where robots and humans coexist in a utopian harmony, powered by a network of self-sustaining energy grids and advanced artificial intelligence.\n",
      "\n",
      "Top-k = 50 (More options)\n",
      "\n",
      "In the year 2154, the once-thriving metropolis of New Eden has evolved into a dazzling, vertically-integrated megastructure, where towering skyscrapers made of gleaming white crystal and iridescent holographic architecture stretch towards the sky, enveloped in a network of sleek, high-speed transportation systems, advanced artificial intelligence networks, and an atmosphere of sustainable luxury, all while being powered by a revolutionary new source of clean energy.\n"
     ]
    }
   ],
   "source": [
    "creative_prompt = \"Describe a futuristic city in one sentence.\"\n",
    "\n",
    "print(\"Top-k = 1 (Most probable only)\")\n",
    "print(query_ollama(creative_prompt, top_k=1))\n",
    "print(\"\\nTop-k = 10 (Limited diversity)\")\n",
    "print(query_ollama(creative_prompt, top_k=10))\n",
    "print(\"\\nTop-k = 50 (More options)\")\n",
    "print(query_ollama(creative_prompt, top_k=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f4745",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "Choose top-k based on your use case:\n",
    "\n",
    "1. **Low Top-k (1-5)**\n",
    "   - When you need very focused, deterministic outputs\n",
    "   - For tasks requiring high precision\n",
    "   - When consistency is crucial\n",
    "\n",
    "2. **Medium Top-k (10-20)**\n",
    "   - For balanced text generation\n",
    "   - When some variation is desired\n",
    "   - For general conversation\n",
    "\n",
    "3. **High Top-k (40-100)**\n",
    "   - For creative writing\n",
    "   - When exploring different possibilities\n",
    "   - For brainstorming sessions\n",
    "\n",
    "**Tips:**\n",
    "- Can be combined with temperature and top-p\n",
    "- Start with top-k = 40 for general use\n",
    "- Lower values create more focused but potentially repetitive text\n",
    "- Higher values allow for more diverse outputs but may include less relevant tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
