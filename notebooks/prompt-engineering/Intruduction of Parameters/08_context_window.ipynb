{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10820866",
   "metadata": {},
   "source": [
    "# Context Window Parameter in LLMs\n",
    "\n",
    "## Introduction\n",
    "The context window (or context length) determines how much text the model can process in a single operation. It represents the maximum number of tokens that can be included in both the input prompt and the generated output combined.\n",
    "\n",
    "Key aspects:\n",
    "- **Measurement**: In tokens (roughly 4 characters per token in English)\n",
    "- **Model-specific**: Varies by model architecture (e.g., 2048, 4096, 8192 tokens)\n",
    "- **Memory usage**: Larger windows require more computational resources\n",
    "- **Attention span**: Affects model's ability to maintain coherence\n",
    "\n",
    "Understanding context window is crucial for:\n",
    "- Processing long documents\n",
    "- Maintaining conversation history\n",
    "- Managing resource usage\n",
    "- Chunking strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def query_ollama(prompt):\n",
    "    \"\"\"Query Ollama with different context lengths\"\"\"\n",
    "    cmd = [\n",
    "        \"curl\",\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        \"-d\",\n",
    "        json.dumps({\n",
    "            \"model\": \"llama3\",\n",
    "            \"prompt\": prompt\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    process = Popen(cmd, stdout=PIPE, stderr=PIPE)\n",
    "    output, _ = process.communicate()\n",
    "\n",
    "    responses = [json.loads(line) for line in output.decode().strip().split(\"\\n\")]\n",
    "    return \"\".join(r.get(\"response\", \"\") for r in responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14083fc",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's explore how context length affects the model's ability to process and maintain information. We'll use prompts of different lengths to demonstrate the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short context example\n",
    "short_prompt = \"Summarize this sentence: The quick brown fox jumps over the lazy dog.\"\n",
    "print(\"Short Context Example:\")\n",
    "print(query_ollama(short_prompt))\n",
    "\n",
    "# Medium context example\n",
    "medium_prompt = \"\"\"Read this paragraph and answer the question at the end:\n",
    "Artificial Intelligence has transformed many industries in the past decade. From healthcare\n",
    "to transportation, AI systems are making processes more efficient and accurate. Machine\n",
    "learning models can now diagnose diseases, drive cars, and even compose music.\n",
    "Question: What are three industries mentioned in the text?\"\"\"\n",
    "print(\"\\nMedium Context Example:\")\n",
    "print(query_ollama(medium_prompt))\n",
    "\n",
    "# Long context example with multiple questions\n",
    "long_prompt = \"\"\"Read this text and answer all questions at the end:\n",
    "The history of computing spans multiple centuries. The first mechanical calculator,\n",
    "the abacus, was invented around 2400 BC. In the 1800s, Charles Babbage designed the\n",
    "Analytical Engine, considered the first general-purpose computer. The modern computer\n",
    "era began in the 1940s with ENIAC, the first electronic general-purpose computer.\n",
    "In the 1970s, personal computers revolutionized computing, making it accessible to\n",
    "everyone. The Internet emerged in the 1990s, connecting computers globally.\n",
    "\n",
    "Questions:\n",
    "1. What was the first mechanical calculator?\n",
    "2. Who designed the Analytical Engine?\n",
    "3. When was ENIAC created?\n",
    "4. What happened in the 1970s?\n",
    "5. What major development occurred in the 1990s?\"\"\"\n",
    "print(\"\\nLong Context Example:\")\n",
    "print(query_ollama(long_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89b343",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "Optimize context window usage based on your use case:\n",
    "\n",
    "1. **Document Processing**\n",
    "   - Chunk long documents strategically\n",
    "   - Maintain overlap between chunks\n",
    "   - Consider summarization for very long texts\n",
    "\n",
    "2. **Conversation Management**\n",
    "   - Implement conversation pruning\n",
    "   - Summarize historical context\n",
    "   - Track token usage\n",
    "\n",
    "3. **Resource Optimization**\n",
    "   - Balance context size and performance\n",
    "   - Consider costs (compute and financial)\n",
    "   - Monitor memory usage\n",
    "\n",
    "**Tips:**\n",
    "- Always check model's maximum context window\n",
    "- Include only relevant context\n",
    "- Use efficient token counting\n",
    "- Implement fallback strategies\n",
    "- Consider using sliding windows\n",
    "- Test with various input lengths"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
