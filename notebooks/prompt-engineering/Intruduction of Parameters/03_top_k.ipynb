{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd49bbb0",
   "metadata": {},
   "source": [
    "# Top-k Parameter in LLMs\n",
    "\n",
    "## Introduction\n",
    "Top-k is a parameter that limits token selection to the k most likely next tokens. It provides direct control over the size of the candidate pool for next token selection, unlike top-p which works with cumulative probabilities.\n",
    "\n",
    "- **Top-k = 1**: Only the single most probable token is considered (greedy)\n",
    "- **Top-k = 10**: Consider only the 10 most probable tokens\n",
    "- **Top-k = 50**: Consider the 50 most probable tokens\n",
    "\n",
    "This parameter is particularly useful when you want explicit control over how many options the model considers at each step of text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa76893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "def query_ollama(prompt, top_k=40):\n",
    "    \"\"\"Query Ollama with a specific top_k setting\"\"\"\n",
    "    cmd = [\n",
    "        \"curl\",\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        \"-d\",\n",
    "        json.dumps({\n",
    "            \"model\": \"llama3\",\n",
    "            \"prompt\": prompt,\n",
    "            \"options\" { \"top_k\": top_k }\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    process = Popen(cmd, stdout=PIPE, stderr=PIPE)\n",
    "    output, _ = process.communicate()\n",
    "\n",
    "    responses = [json.loads(line) for line in output.decode().strip().split(\"\\n\")]\n",
    "    return \"\".join(r.get(\"response\", \"\") for r in responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e228f",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's explore how different top-k values affect the model's output. We'll use a creative task to demonstrate how limiting token selection impacts generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7feb4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k = 1 (Most probable only)\n",
      "In the year 2154, the metropolis of New Eden sprawled across the horizon, its towering skyscrapers and gleaming spires piercing the sky like shards of glass, while flying cars zipped through crystal-clear air tunnels, surrounded by lush greenery and holographic advertisements that danced in mid-air.\n",
      "\n",
      "Top-k = 10 (Limited diversity)\n",
      "The city of New Eden sprawls across the horizon, its gleaming skyscrapers and intricate networks of hovering walkways and hyperloops piercing the sky like shards of crystal, as holographic advertisements and self-driving vehicles weave through the streets with precision and ease.\n",
      "\n",
      "Top-k = 50 (More options)\n",
      "New Eden, a sprawling metropolis of gleaming skyscrapers and iridescent domes, hummed with the gentle thrum of hovercraft and levitating cars, its streets lined with holographic advertisements and virtual reality alleys that blurred the lines between technology and artistry.\n"
     ]
    }
   ],
   "source": [
    "creative_prompt = \"Describe a futuristic city in one sentence.\"\n",
    "\n",
    "print(\"Top-k = 1 (Most probable only)\")\n",
    "print(query_ollama(creative_prompt, top_k=1))\n",
    "print(\"\\nTop-k = 10 (Limited diversity)\")\n",
    "print(query_ollama(creative_prompt, top_k=10))\n",
    "print(\"\\nTop-k = 50 (More options)\")\n",
    "print(query_ollama(creative_prompt, top_k=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f4745",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "Choose top-k based on your use case:\n",
    "\n",
    "1. **Low Top-k (1-5)**\n",
    "   - When you need very focused, deterministic outputs\n",
    "   - For tasks requiring high precision\n",
    "   - When consistency is crucial\n",
    "\n",
    "2. **Medium Top-k (10-20)**\n",
    "   - For balanced text generation\n",
    "   - When some variation is desired\n",
    "   - For general conversation\n",
    "\n",
    "3. **High Top-k (40-100)**\n",
    "   - For creative writing\n",
    "   - When exploring different possibilities\n",
    "   - For brainstorming sessions\n",
    "\n",
    "**Tips:**\n",
    "- Can be combined with temperature and top-p\n",
    "- Start with top-k = 40 for general use\n",
    "- Lower values create more focused but potentially repetitive text\n",
    "- Higher values allow for more diverse outputs but may include less relevant tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
